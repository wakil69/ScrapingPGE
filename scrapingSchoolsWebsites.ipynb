{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3af7d82",
   "metadata": {},
   "source": [
    "# Scraping Websites\n",
    "\n",
    "Notes:\n",
    "\n",
    "1) Importing the first_step_name_and_description_url.xlsx (\"name\" and \"description url\")\n",
    "2) Detecting the website of each school and the page with the \"Organigramme\" in DuckDuckGo, we make two searches : \"{name}\" and \"{name} organigramme\"\n",
    "3) We save the results in excel\n",
    "4) Browsing the website to find emails and particularly domain name\n",
    "5) From the page organigramme, we fetch the emails and the names of each person. If there is the email associated to each person -> ok\n",
    "If there is no email associated to each person -> we build the email and we test them.\n",
    "6) We save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from selenium import webdriver\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    StaleElementReferenceException,\n",
    "    NoSuchElementException,\n",
    "    WebDriverException,\n",
    ")\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "\n",
    "import os\n",
    "import random\n",
    "import psutil\n",
    "import time \n",
    "import re \n",
    "import pandas as pd \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de51fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_seek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "qwen_api_key = os.getenv(\"QWEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebbf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import psutil\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "def thread_worker(row):\n",
    "    \"\"\"Each thread creates its own FetchWebsites instance + driver.\"\"\"\n",
    "    fetcher = FetchWebsites()\n",
    "    name = row[\"name\"]\n",
    "    desc_url = row[\"url\"]\n",
    "\n",
    "    try:\n",
    "        time.sleep(random.uniform(1, 5))  # stagger startup\n",
    "        print(f\"[Thread] üîç Searching for: {name}\")\n",
    "\n",
    "        website = fetcher.search_duckduckgo(name)\n",
    "            \n",
    "        print(f\"Found Website: {website}\")\n",
    "\n",
    "        fetcher._human_delay(2, 5)\n",
    "\n",
    "        organigramme = fetcher.search_duckduckgo(f\"{name} organigramme\")\n",
    "\n",
    "        print(f\"Found Organigramme: {organigramme}\")\n",
    "            \n",
    "        return {\n",
    "            \"index\": row.name,\n",
    "            \"name\": name,\n",
    "            \"description url\": desc_url,\n",
    "            \"website\": website,\n",
    "            \"organigramme_page\": organigramme\n",
    "        }\n",
    "    finally:\n",
    "        fetcher.clean_selenium()\n",
    "\n",
    "\n",
    "class FetchWebsites:\n",
    "    def __init__(self):\n",
    "        self.user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_0_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 12_6_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15\"\n",
    "        ]\n",
    "        self.driver = self._init_browser()\n",
    "\n",
    "    def _init_browser(self):\n",
    "        \"\"\"Initialize Selenium Chrome driver with random user agent.\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        random_user_agent = random.choice(self.user_agents)\n",
    "        chrome_options.add_argument(f\"user-agent={random_user_agent}\")\n",
    "        return webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    def clean_selenium(self):\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error quitting WebDriver: {e}\")\n",
    "\n",
    "        for proc in psutil.process_iter():\n",
    "            try:\n",
    "                if proc.name().lower() in [\n",
    "                    \"chromedriver\", \"chrome\", \"google-chrome\",\n",
    "                    \"chromium\", \"geckodriver\", \"msedgedriver\", \"chrome_crashpad\"\n",
    "                ]:\n",
    "                    proc.kill()\n",
    "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                continue\n",
    "\n",
    "    def _human_delay(self, min_t=0.5, max_t=1.5):\n",
    "        time.sleep(random.uniform(min_t, max_t))\n",
    "\n",
    "    def _human_scroll(self):\n",
    "        scroll_height = random.randint(200, 600)\n",
    "        self.driver.execute_script(f\"window.scrollBy(0, {scroll_height});\")\n",
    "        self._human_delay(0.5, 1)\n",
    "\n",
    "    def _human_mouse_move(self):\n",
    "        actions = ActionChains(self.driver)\n",
    "        body = self.driver.find_element(By.TAG_NAME, \"body\")\n",
    "        actions.move_to_element_with_offset(body, random.randint(0, 500), random.randint(0, 300)).perform()\n",
    "        self._human_delay(0.3, 0.7)\n",
    "\n",
    "    def search_duckduckgo(self, query, retries=2):\n",
    "        for attempt in range(retries + 1):\n",
    "            try:\n",
    "                self.driver.get(\"https://duckduckgo.com/?kl=fr-fr\")\n",
    "                self._human_delay(1, 2)\n",
    "                box = self.driver.find_element(By.NAME, \"q\")\n",
    "                box.clear()\n",
    "                for char in query:\n",
    "                    box.send_keys(char)\n",
    "                    self._human_delay(0.05, 0.2)\n",
    "                box.send_keys(Keys.RETURN)\n",
    "                self._human_delay(2, 3)\n",
    "                self._human_scroll()\n",
    "                if random.random() > 0.5:\n",
    "                    self._human_mouse_move()\n",
    "                results = self.driver.find_elements(By.CSS_SELECTOR, \"li[data-layout='organic'] a[data-testid='result-title-a']\")\n",
    "                if results:\n",
    "                    return results[0].get_attribute(\"href\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Search failed for '{query}' attempt {attempt+1}: {e}\")\n",
    "                self.driver = self._init_browser()\n",
    "                self._human_delay(2, 4)\n",
    "        return None\n",
    "\n",
    "    def run(self, input_file, output_file, max_workers=3):\n",
    "        df = pd.read_excel(input_file)\n",
    "        nb_rows = len(df)\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            out_df = pd.read_excel(output_file)\n",
    "            done_idx = set(out_df[\"index\"])\n",
    "            results = out_df.to_dict(\"records\")\n",
    "            print(f\"‚ñ∂Ô∏è Resuming from {len(done_idx)}/{nb_rows} already completed.\")\n",
    "        else:\n",
    "            results, done_idx = [], set()\n",
    "\n",
    "        pending = df[~df.index.isin(done_idx)]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(thread_worker, row): row for _, row in pending.iterrows()}\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                row = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    tmp_file = output_file + \".tmp\"\n",
    "                    pd.DataFrame(results).to_excel(tmp_file, index=False)\n",
    "                    os.replace(tmp_file, output_file) \n",
    "                    print(f\"üíæ Saved {len(results)}/{nb_rows} (just finished {row['name']})\")\n",
    "                    time.sleep(random.uniform(1, 3))  # small cooldown\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error processing {row['name']}: {e}\")\n",
    "\n",
    "        print(f\"‚úÖ All results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eee136",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetchWebsites = FetchWebsites()\n",
    "\n",
    "fetchWebsites.run(\"first_step_name_and_description_url.xlsx\", \"websites_schools.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_emails(self, text):\n",
    "#     \"\"\"Extracts and filters valid emails from the given text.\"\"\"\n",
    "#     email_regex = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "#     possible_emails = set(re.findall(email_regex, text))\n",
    "#     valid_tlds = {\n",
    "#         \"com\",\n",
    "#         \"org\",\n",
    "#         \"net\",\n",
    "#         \"edu\",\n",
    "#         \"gov\",\n",
    "#         \"eu\",\n",
    "#         \"br\",\n",
    "#         \"fr\",\n",
    "#         \"de\",\n",
    "#         \"es\",\n",
    "#         \"pl\",\n",
    "#         \"it\",\n",
    "#         \"uk\",\n",
    "#         \"ru\",\n",
    "#         \"in\",\n",
    "#         \"ch\",\n",
    "#     }\n",
    "#     invalid_extensions = {\"jpg\", \"jpeg\", \"png\", \"gif\", \"webp\", \"bmp\", \"svg\", \"tiff\"}\n",
    "\n",
    "#     valid_emails = set()\n",
    "\n",
    "#     for email in possible_emails:\n",
    "#         domain_parts = email.split(\".\")\n",
    "#         tld = domain_parts[-1].lower()\n",
    "\n",
    "#         if tld not in invalid_extensions:\n",
    "#             valid_emails.add(email)\n",
    "\n",
    "#     return valid_emails\n",
    "\n",
    "# def extract_visible_text(html):\n",
    "#     soup = BeautifulSoup(html, \"html.parser\")\n",
    "#     for script in soup([\"script\", \"style\"]):\n",
    "#         script.decompose()\n",
    "#     return soup.get_text(separator=\" \")\n",
    "\n",
    "# def crawl_site_depth(base_url, max_depth=1):\n",
    "#     \"\"\"\n",
    "#     Crawls a site using Selenium to extract emails, subpages, and external links.\n",
    "#     Limits crawling to `max_depth` hierarchical levels.\n",
    "#     \"\"\"\n",
    "#     visited_subpages = set()\n",
    "#     queue = deque([(base_url, 0)])\n",
    "#     failed_urls = set()\n",
    "\n",
    "#     while queue:\n",
    "#         url, depth = queue.popleft()\n",
    "#         normalized_url = url.split(\"#\")[0]\n",
    "        \n",
    "#         print(normalized_url)\n",
    "\n",
    "#         if normalized_url in visited_subpages or depth > max_depth:\n",
    "#             continue\n",
    "\n",
    "#         try:\n",
    "#             driver.set_page_load_timeout(10)\n",
    "#             driver.get(normalized_url)\n",
    "            \n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "#             )\n",
    "            \n",
    "\n",
    "#             driver.execute_script(\n",
    "#                 \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "#             )\n",
    "        \n",
    "#             time.sleep(2)\n",
    "                            \n",
    "#             html = driver.page_source\n",
    "#             visited_subpages.add(normalized_url)\n",
    "#             soup = BeautifulSoup(html, \"html.parser\")\n",
    "                            \n",
    "#             visible_text = extract_visible_text(html)                \n",
    "#             new_emails = get_emails(visible_text)\n",
    "                            \n",
    "#             if new_emails:\n",
    "#                 print(f\"üìß Emails found: {new_emails}\")\n",
    "#                 emails.update(new_emails)\n",
    "            \n",
    "            \n",
    "#             VIDEO_KEYWORDS = [\"youtube\", \"vimeo\", \"dailymotion\", \"wistia\", \"player.\", \"video\"]\n",
    "            \n",
    "#             for iframe in soup.find_all(\"iframe\", src=True): #job boards in iframe\n",
    "#                 iframe_src = iframe[\"src\"].split(\"#\")[0]\n",
    "#                 iframe_url = urljoin(normalized_url, iframe_src)\n",
    "#                 parsed_iframe_url = urlparse(iframe_url)\n",
    "\n",
    "#                 if any(keyword in iframe_url.lower() for keyword in VIDEO_KEYWORDS):\n",
    "#                     continue\n",
    "                \n",
    "#                 if iframe_url in visited_subpages:\n",
    "#                     continue\n",
    "                \n",
    "#                 if any(iframe_url.endswith(ext) for ext in [\".js\", \".css\", \".jpg\", \".jpeg\", \".png\", \".pdf\"]):\n",
    "#                     continue\n",
    "                \n",
    "#                 if iframe_url.startswith(\"mailto:\") or \"javascript:void\" in iframe_url:\n",
    "#                     continue\n",
    "                \n",
    "#                 if parsed_iframe_url.netloc == urlparse(base_url).netloc:\n",
    "#                     if depth + 1 <= max_depth:\n",
    "#                         queue.append((iframe_url, depth + 1))\n",
    "#                 else:\n",
    "#                     external_urls.add(iframe_url)\n",
    "    \n",
    "#             for link in soup.find_all(\"a\", href=True):\n",
    "#                 # absolute_link = urljoin(base_url, link[\"href\"])\n",
    "#                 href = link[\"href\"].split(\"#\")[0]\n",
    "#                 absolute_link = urljoin(base_url, href)\n",
    "#                 parsed_url = urlparse(absolute_link)\n",
    "\n",
    "#                 if absolute_link in visited_subpages:\n",
    "#                     continue\n",
    "                \n",
    "#                 if any(absolute_link.endswith(ext) for ext in [\".js\", \".css\", \".jpg\", \".jpeg\", \".png\", \".pdf\"]):\n",
    "#                     continue\n",
    "#                 if absolute_link.startswith(\"mailto:\") or \"javascript:void\" in absolute_link:\n",
    "#                     continue\n",
    "\n",
    "#                 if parsed_url.netloc == urlparse(base_url).netloc:\n",
    "#                     if depth + 1 <= max_depth:\n",
    "#                         queue.append((absolute_link, depth + 1))\n",
    "#                 else:\n",
    "#                     external_urls.add(absolute_link)\n",
    "\n",
    "#         except (TimeoutException, WebDriverException) as e:\n",
    "#             print(f\"‚ö†Ô∏è WebDriver error on {normalized_url}: {e}\")\n",
    "            \n",
    "#             if normalized_url not in failed_urls:\n",
    "#                 failed_urls.add(normalized_url)\n",
    "#                 print(\"üîÅ Restarting browser and retrying once...\")\n",
    "#                 restart_browser()\n",
    "#                 queue.append((url, depth))  # Requeue the same URL just once\n",
    "#             else:\n",
    "#                 print(\"‚ùå Already retried once. Skipping permanently.\")\n",
    "#             continue\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ö†Ô∏è Unexpected error loading {url}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     return list(visited_subpages)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scrapingpge)",
   "language": "python",
   "name": "scrapingpge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
