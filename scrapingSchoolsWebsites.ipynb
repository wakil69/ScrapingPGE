{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3af7d82",
   "metadata": {},
   "source": [
    "# Scraping Websites\n",
    "\n",
    "Notes:\n",
    "\n",
    "1) Importing the first_step_name_and_description_url.xlsx (\"name\" and \"description url\")\n",
    "2) Detecting the website of each school and the page with the \"Organigramme\" in DuckDuckGo, we make two searches : \"{name}\" and \"{name} organigramme\"\n",
    "3) We save the results in excel\n",
    "4) Browsing the website to find emails and particularly domain name\n",
    "5) From the page organigramme, we fetch the emails and the names of each person. If there is the email associated to each person -> ok\n",
    "If there is no email associated to each person -> we build the email and we test them.\n",
    "6) We save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ec19e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "from selenium import webdriver\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    StaleElementReferenceException,\n",
    "    NoSuchElementException,\n",
    "    WebDriverException,\n",
    ")\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "import random\n",
    "import psutil\n",
    "import time \n",
    "import re \n",
    "import pandas as pd \n",
    "import json \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7de51fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_seek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "qwen_api_key = os.getenv(\"QWEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebbf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import psutil\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "def thread_worker(row):\n",
    "    \"\"\"Each thread creates its own FetchWebsites instance + driver.\"\"\"\n",
    "    fetcher = FetchWebsites()\n",
    "    name = row[\"name\"]\n",
    "    desc_url = row[\"url\"]\n",
    "\n",
    "    try:\n",
    "        time.sleep(random.uniform(1, 5))  # stagger startup\n",
    "        print(f\"[Thread] üîç Searching for: {name}\")\n",
    "\n",
    "        website = fetcher.search_duckduckgo(name)\n",
    "            \n",
    "        print(f\"Found Website: {website}\")\n",
    "\n",
    "        fetcher._human_delay(2, 5)\n",
    "\n",
    "        organigramme = fetcher.search_duckduckgo(f\"{name} organigramme\")\n",
    "\n",
    "        print(f\"Found Organigramme: {organigramme}\")\n",
    "            \n",
    "        return {\n",
    "            \"index\": row.name,\n",
    "            \"name\": name,\n",
    "            \"description url\": desc_url,\n",
    "            \"website\": website,\n",
    "            \"organigramme_page\": organigramme\n",
    "        }\n",
    "    finally:\n",
    "        fetcher.clean_selenium()\n",
    "\n",
    "\n",
    "class FetchWebsites:\n",
    "    def __init__(self):\n",
    "        self.user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_0_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 12_6_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15\"\n",
    "        ]\n",
    "        self.driver = self._init_browser()\n",
    "\n",
    "    def _init_browser(self):\n",
    "        \"\"\"Initialize Selenium Chrome driver with random user agent.\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        random_user_agent = random.choice(self.user_agents)\n",
    "        chrome_options.add_argument(f\"user-agent={random_user_agent}\")\n",
    "        return webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    def clean_selenium(self):\n",
    "        if getattr(self, \"driver\", None):\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error quitting WebDriver: {e}\")\n",
    "\n",
    "    def _human_delay(self, min_t=0.5, max_t=1.5):\n",
    "        time.sleep(random.uniform(min_t, max_t))\n",
    "\n",
    "    def _human_scroll(self):\n",
    "        scroll_height = random.randint(200, 600)\n",
    "        self.driver.execute_script(f\"window.scrollBy(0, {scroll_height});\")\n",
    "        self._human_delay(0.5, 1)\n",
    "\n",
    "    def _human_mouse_move(self):\n",
    "        actions = ActionChains(self.driver)\n",
    "        body = self.driver.find_element(By.TAG_NAME, \"body\")\n",
    "        actions.move_to_element_with_offset(body, random.randint(0, 500), random.randint(0, 300)).perform()\n",
    "        self._human_delay(0.3, 0.7)\n",
    "\n",
    "    def search_duckduckgo(self, query, retries=2):\n",
    "        for attempt in range(retries + 1):\n",
    "            try:\n",
    "                # self.driver.get(\"https://duckduckgo.com/?kl=fr-fr\")\n",
    "                self._human_delay(1, 2)\n",
    "                box = self.driver.find_element(By.NAME, \"q\")\n",
    "                box.clear()\n",
    "                for char in query:\n",
    "                    box.send_keys(char)\n",
    "                    self._human_delay(0.05, 0.2)\n",
    "                box.send_keys(Keys.RETURN)\n",
    "                self._human_delay(2, 3)\n",
    "                self._human_scroll()\n",
    "                if random.random() > 0.5:\n",
    "                    self._human_mouse_move()\n",
    "                results = self.driver.find_elements(By.CSS_SELECTOR, \"li[data-layout='organic'] a[data-testid='result-title-a']\")\n",
    "                if results:\n",
    "                    return results[0].get_attribute(\"href\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Search failed for '{query}' attempt {attempt+1}: {e}\")\n",
    "                self.driver = self._init_browser()\n",
    "                self._human_delay(2, 4)\n",
    "        return None\n",
    "\n",
    "    def run(self, input_file, output_file, max_workers=3):\n",
    "        df = pd.read_excel(input_file)\n",
    "        nb_rows = len(df)\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            out_df = pd.read_excel(output_file)\n",
    "            done_idx = set(out_df[\"index\"])\n",
    "            results = out_df.to_dict(\"records\")\n",
    "            print(f\"‚ñ∂Ô∏è Resuming from {len(done_idx)}/{nb_rows} already completed.\")\n",
    "        else:\n",
    "            results, done_idx = [], set()\n",
    "\n",
    "        pending = df[~df.index.isin(done_idx)]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(thread_worker, row): row for _, row in pending.iterrows()}\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                row = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    pd.DataFrame(results).to_excel(output_file, index=False)\n",
    "                    print(f\"üíæ Saved {len(results)}/{nb_rows} (just finished {row['name']})\")\n",
    "                    time.sleep(random.uniform(1, 3))  # small cooldown\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error processing {row['name']}: {e}\")\n",
    "\n",
    "        print(f\"‚úÖ All results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eee136",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetchWebsites = FetchWebsites()\n",
    "\n",
    "fetchWebsites.run(\"first_step_name_and_description_url.xlsx\", \"websites_schools.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapingStaff:\n",
    "    def __init__(self, website, website_staff):\n",
    "        self.user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_0_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 12_6_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15\",\n",
    "        ]\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        random_user_agent = random.choice(self.user_agents)\n",
    "        chrome_options.add_argument(f\"user-agent={random_user_agent}\")\n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.emails = set()\n",
    "        self.qwen_client = OpenAI(\n",
    "            api_key=qwen_api_key,\n",
    "            base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\",\n",
    "        )\n",
    "        self.website = website\n",
    "        self.website_staff = website_staff\n",
    "        self.visited_subpages = set()\n",
    "\n",
    "    def restart_browser(self):\n",
    "        \"\"\"Fully restarts the Selenium browser.\"\"\"\n",
    "        self.clean_selenium()\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        random_user_agent = random.choice(self.user_agents)\n",
    "        chrome_options.add_argument(f\"user-agent={random_user_agent}\")\n",
    "\n",
    "        # if os.getuid() == 0:\n",
    "        #     chrome_options.add_argument(\"--disable-gpu\")\n",
    "        #     chrome_options.add_argument(\"--remote-debugging-port=9222\")\n",
    "\n",
    "        self.driver = webdriver.Chrome(\n",
    "            options=chrome_options\n",
    "        )\n",
    "\n",
    "    def clean_selenium(self):\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()  # Gracefully quit the browser\n",
    "            except Exception as e:\n",
    "                print(f\"Error quitting WebDriver: {e}\")\n",
    "\n",
    "        # Kill and properly reap any remaining browser processes\n",
    "        for proc in psutil.process_iter():\n",
    "            try:\n",
    "                if proc.name().lower() in [\n",
    "                    \"chromedriver\",\n",
    "                    \"chrome\",\n",
    "                    \"google-chrome\",\n",
    "                    \"chromium\",\n",
    "                    \"geckodriver\",\n",
    "                    \"msedgedriver\",\n",
    "                    \"chrome_crashpad\",  # include this too\n",
    "                    \"cat\",  # since you're seeing this too\n",
    "                ]:\n",
    "                    proc.kill()\n",
    "                    try:\n",
    "                        proc.wait(timeout=5)  # ‚úÖ Reap the process\n",
    "                    except (psutil.TimeoutExpired, psutil.NoSuchProcess):\n",
    "                        pass\n",
    "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                continue\n",
    "\n",
    "    def get_emails(self, text):\n",
    "        email_regex = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "        possible_emails = set(re.findall(email_regex, text))\n",
    "        invalid_extensions = {\"jpg\", \"jpeg\", \"png\", \"gif\", \"webp\", \"bmp\", \"svg\", \"tiff\", \"pdf\"}\n",
    "        valid_emails = set()\n",
    "        for email in possible_emails:\n",
    "            tld = email.rsplit(\".\", 1)[-1].lower()\n",
    "            if tld not in invalid_extensions:\n",
    "                valid_emails.add(email)\n",
    "        return valid_emails\n",
    "\n",
    "    def extract_visible_text(self, html):\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for script in soup([\"script\", \"style\", \"meta\", \"svg\", \"noscript\"]):\n",
    "            script.decompose()\n",
    "        return soup.get_text(separator=\" \")\n",
    "\n",
    "    def crawl_site_depth(self, max_depth=1):\n",
    "        \"\"\"\n",
    "        Crawls a site using Selenium to extract emails, subpages, and external links.\n",
    "        Limits crawling to `max_depth` hierarchical levels.\n",
    "        \"\"\"\n",
    "        queue = deque([(self.website, 0)])\n",
    "        failed_urls = set()\n",
    "\n",
    "        while queue:\n",
    "            url, depth = queue.popleft()\n",
    "            normalized_url = url.split(\"#\")[0]\n",
    "\n",
    "            if normalized_url in self.visited_subpages or depth > max_depth:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                self.driver.set_page_load_timeout(10)\n",
    "                self.driver.get(normalized_url)\n",
    "\n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                )\n",
    "\n",
    "                self.driver.execute_script(\n",
    "                    \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "                )\n",
    "\n",
    "                time.sleep(2)\n",
    "\n",
    "                html = self.driver.page_source\n",
    "                self.visited_subpages.add(normalized_url)\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                visible_text = self.extract_visible_text(html)\n",
    "                new_emails = self.get_emails(visible_text)\n",
    "\n",
    "                if new_emails:\n",
    "                    print(f\"üìß Emails found: {new_emails}\")\n",
    "                    self.emails.update(new_emails)\n",
    "\n",
    "                for link in soup.find_all(\"a\", href=True):\n",
    "                    href = link[\"href\"].split(\"#\")[0]\n",
    "                    absolute_link = urljoin(self.website, href)\n",
    "                    parsed_url = urlparse(absolute_link)\n",
    "\n",
    "                    if absolute_link in self.visited_subpages:\n",
    "                        continue\n",
    "                    \n",
    "                    if any(absolute_link.endswith(ext) for ext in [\".js\", \".css\", \".jpg\", \".jpeg\", \".png\", \".pdf\"]):\n",
    "                        continue\n",
    "                    if absolute_link.startswith(\"mailto:\") or \"javascript:void\" in absolute_link:\n",
    "                        continue\n",
    "\n",
    "                    if parsed_url.netloc == urlparse(self.website).netloc:\n",
    "                        if depth + 1 <= max_depth:\n",
    "                            print(absolute_link)\n",
    "                            queue.append((absolute_link, depth + 1))\n",
    "\n",
    "\n",
    "            except (TimeoutException, WebDriverException) as e:\n",
    "                print(f\"‚ö†Ô∏è WebDriver error on {normalized_url}: {e}\")\n",
    "\n",
    "                if normalized_url not in failed_urls:\n",
    "                    failed_urls.add(normalized_url)\n",
    "                    print(\"üîÅ Restarting browser and retrying once...\")\n",
    "                    self.restart_browser()\n",
    "                    queue.append((url, depth))  # Requeue the same URL just once\n",
    "                else:\n",
    "                    print(\"‚ùå Already retried once. Skipping permanently.\")\n",
    "                continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Unexpected error loading {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return\n",
    "\n",
    "    def find_url_staff_llm(self):\n",
    "\n",
    "        print(self.visited_subpages)\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "                    You are an assistant that selects the most relevant webpage URL for finding staff or organizational structure information.\n",
    "\n",
    "                    From the provided list of URLs, return ONLY the single best candidate URL that is most likely to contain staff, team, personnel, or organigramme information.\n",
    "\n",
    "                    - Output must be valid JSON in this format:\n",
    "\n",
    "                    {\n",
    "                    \"staff_url\": \"https://example.com/...\"\n",
    "                    }\n",
    "\n",
    "                    - If no suitable URL is found, return:\n",
    "\n",
    "                    {\n",
    "                    \"staff_url\": \"\"\n",
    "                    }\n",
    "\n",
    "                    Do not include explanations or any text outside the JSON.\n",
    "        \"\"\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Visited Urls:\\n{self.visited_subpages}\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        staff_url_obj = {\"staff_url\": \"\"}\n",
    "        \n",
    "        try:\n",
    "\n",
    "            llm_response = self.qwen_client.chat.completions.create(\n",
    "                model=\"qwen-plus\",\n",
    "                messages=messages,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0,\n",
    "                max_tokens=8192,\n",
    "            )\n",
    "\n",
    "            content = llm_response.choices[0].message.content.strip()\n",
    "            staff_url_obj = json.loads(content)\n",
    "            print(staff_url_obj, \"yeahhhhh\")\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                llm_response = self.qwen_client.chat.completions.create(\n",
    "                    model=\"qwen-plus\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"\"\"\n",
    "                        You are an AI specialized in returning JSON in a valid format.\n",
    "                        Do NOT modify the content‚Äîonly ensure the JSON is properly formatted.\n",
    "                        \"\"\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"Fix the JSON format:\\n{llm_response.choices[0].message.content.strip()}\",\n",
    "                        },\n",
    "                    ],\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    temperature=0,\n",
    "                    max_tokens=8192,\n",
    "                )\n",
    "\n",
    "                content = llm_response.choices[0].message.content.strip()\n",
    "                staff_url_obj = json.loads(content)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è LLM JSON Parsing Error: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing {self.website_staff}: {e}\")\n",
    "            \n",
    "        return staff_url_obj.get(\"staff_url\") or \"\"\n",
    "\n",
    "    def extract_staff_llm(self):\n",
    "\n",
    "        self.driver.get(self.website_staff)\n",
    "\n",
    "        WebDriverWait(self.driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "\n",
    "        html = self.driver.page_source\n",
    "\n",
    "        visible_text = self.extract_visible_text(html)\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "                You are an information extraction assistant. \n",
    "                Your task is to analyze the provided webpage text and extract a list of staff members.\n",
    "\n",
    "                Return the result in valid JSON with the following structure only:\n",
    "\n",
    "                {\n",
    "                \"staff\": [\n",
    "                    {\"name\": \"Full Name\", \"function\": \"Job Title or Role\", \"email\": \"Email\"},\n",
    "                    ...\n",
    "                ]\n",
    "                }\n",
    "\n",
    "                - If any field is missing (e.g., no email), leave it as an empty string \"\".\n",
    "                - Do not include extra text, explanations, or formatting outside of the JSON.\n",
    "                - Preserve names and job titles as they appear in the text.\n",
    "                - Extract only staff-related information (ignore unrelated text).\n",
    "        \"\"\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"PAGE TEXT:\\n{visible_text}\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        result = {\"staff\": []}\n",
    "        \n",
    "        try:\n",
    "\n",
    "            llm_response = self.qwen_client.chat.completions.create(\n",
    "                model=\"qwen-plus\",\n",
    "                messages=messages,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0,\n",
    "                max_tokens=8192,\n",
    "            )\n",
    "\n",
    "            content = llm_response.choices[0].message.content.strip()\n",
    "            result = json.loads(content)\n",
    "            if not isinstance(result, dict) or \"staff\" not in result:\n",
    "                result = {\"staff\": []}\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                llm_response = self.qwen_client.chat.completions.create(\n",
    "                    model=\"qwen-plus\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"\"\"\n",
    "                        You are an AI specialized in returning JSON in a valid format.\n",
    "                        Do NOT modify the content‚Äîonly ensure the JSON is properly formatted.\n",
    "                        \"\"\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"Fix the JSON format:\\n{llm_response.choices[0].message.content.strip()}\",\n",
    "                        },\n",
    "                    ],\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    temperature=0,\n",
    "                    max_tokens=8192,\n",
    "                )\n",
    "\n",
    "                content = llm_response.choices[0].message.content.strip()\n",
    "                result = json.loads(content)\n",
    "                if not isinstance(result, dict) or \"staff\" not in result:\n",
    "                    result = {\"staff\": []}\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è LLM JSON Parsing Error: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing {self.website_staff}: {e}\")\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def extract_staff(self):\n",
    "        results = []\n",
    "\n",
    "        def same_root(url1, url2):\n",
    "            \"\"\"Check if two URLs belong to the same root domain.\"\"\"\n",
    "            netloc1 = urlparse(url1).netloc.lower()\n",
    "            netloc2 = urlparse(url2).netloc.lower()\n",
    "            return netloc1 == netloc2 or netloc1.endswith(\".\" + netloc2) or netloc2.endswith(\".\" + netloc1)\n",
    "        \n",
    "        # 1) First try current website_staff\n",
    "        staff_data = {}\n",
    "        if self.website_staff and same_root(self.website, self.website_staff):\n",
    "            print(\"NOPE\")\n",
    "            staff_data = self.extract_staff_llm()\n",
    "            \n",
    "        # 2) If no staff OR website_staff outside root ‚Üí ask LLM to find a better page\n",
    "        if (not isinstance(staff_data, dict) or not staff_data.get(\"staff\")):\n",
    "            # Try finding a better staff page and re-run once\n",
    "            candidate = self.find_url_staff_llm()\n",
    "            print(\"candidate : \", candidate)\n",
    "            if candidate:\n",
    "                self.website_staff = candidate\n",
    "                staff_data = self.extract_staff_llm()\n",
    "\n",
    "        if isinstance(staff_data, dict) and staff_data.get(\"staff\"):\n",
    "            for person in staff_data[\"staff\"]:\n",
    "                if isinstance(person, dict) and person.get(\"name\"):\n",
    "                    # Ensure required keys exist\n",
    "                    results.append({\n",
    "                        \"name\": person.get(\"name\", \"\").strip(),\n",
    "                        \"function\": person.get(\"function\", \"\").strip(),\n",
    "                        \"email\": person.get(\"email\", \"\").strip(),\n",
    "                    })\n",
    "                    \n",
    "        return results        \n",
    "    \n",
    "    def get_most_common_domain(self):\n",
    "        domains = []\n",
    "        for email in self.emails:\n",
    "            if \"@\" in email:\n",
    "                domain = email.split(\"@\")[1].lower().strip()\n",
    "                domains.append(domain)\n",
    "        \n",
    "        if not domains:\n",
    "            return None, 0\n",
    "        \n",
    "        counter = Counter(domains)\n",
    "        most_common_domain, count = counter.most_common(1)[0]\n",
    "        return most_common_domain, count\n",
    "\n",
    "    def __call__(self):\n",
    "        print(self.website, self.website_staff)\n",
    "        \n",
    "        self.crawl_site_depth()\n",
    "        \n",
    "        staff = self.extract_staff()\n",
    "        \n",
    "        print(staff)\n",
    "        \n",
    "        print(self.emails)\n",
    "        \n",
    "        domain, count = self.get_most_common_domain()\n",
    "        \n",
    "        print(domain, count)\n",
    "        \n",
    "        self.clean_selenium()\n",
    "        \n",
    "        return staff, domain, self.emails\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_schools = pd.read_excel(\"websites_schools.xlsx\")\n",
    "\n",
    "# Add empty columns for results\n",
    "websites_schools[\"staff\"] = None\n",
    "websites_schools[\"domain\"] = None\n",
    "websites_schools[\"emails\"] = None\n",
    "\n",
    "for idx, row in websites_schools.iterrows():\n",
    "    name = row[\"name\"]\n",
    "    description_url = row[\"description url\"]\n",
    "    website = row[\"website\"]\n",
    "    website_staff = row[\"organigramme_page\"]\n",
    "\n",
    "    if idx == 1:  # limit for testing\n",
    "        scraping_staff = ScrapingStaff(website, website_staff)\n",
    "        staff, domain, emails = scraping_staff()\n",
    "        print(staff, domain, emails)\n",
    "\n",
    "        # save results into DataFrame\n",
    "        websites_schools.at[idx, \"staff\"] = str(staff)   # store as JSON-like string\n",
    "        websites_schools.at[idx, \"domain\"] = domain\n",
    "        websites_schools.at[idx, \"emails\"] = str(emails)\n",
    "\n",
    "        # save to file after each loop\n",
    "        websites_schools.to_excel(\"websites_schools_results.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scrapingpge)",
   "language": "python",
   "name": "scrapingpge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
